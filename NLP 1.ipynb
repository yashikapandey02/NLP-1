{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ff34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Explain One-Hot Encoding ?\n",
    "\n",
    "Ans: One Hot Encoding is a process by which categorical variables are converted into a form that could be provided to ML\n",
    "    algorithms to do a better job in prediction.\n",
    "\n",
    "One hot encoding is the most widespread approach, and it works very well unless your categorical variable takes on a large \n",
    "number of values (i.e. you generally won't it for variables taking more than 15 different values. It'd be a poor choice in \n",
    "                  some cases with fewer values, though that varies.)\n",
    "\n",
    "One hot encoding creates new (binary) columns, indicating the presence of each possible value from the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d08b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Explain Bag of Words ?\n",
    "Ans: The bag-of-words (BOW) model is a representation that turns arbitrary text into fixed-length vectors by counting how many\n",
    "    times each word appears. This process is often referred to as vectorization.\n",
    "    \n",
    "    Suppose we wanted to vectorize the following:\n",
    "the cat sat\n",
    "the cat sat in the hat\n",
    "the cat with the hat\n",
    "\n",
    "We’ll refer to each of these as a text document.\n",
    "\n",
    "We first define our vocabulary, which is the set of all words found in our document set. The only words that are found in the \n",
    "3 documents above are: the, cat, sat, in, the, hat, and with.\n",
    "To vectorize our documents, all we have to do is count how many times each word appears:\n",
    "\n",
    "Now we have length-6 vectors for each document!\n",
    "the cat sat: [1, 1, 1, 0, 0, 0]\n",
    "the cat sat in the hat: [2, 1, 1, 1, 1, 0]\n",
    "the cat with the hat: [2, 1, 0, 0, 1, 1]\n",
    "\n",
    "The Problem with BOW is, it does not presever contextual infomration among the words. Notice that we lose contextual \n",
    "information, e.g. where in the document the word appeared, when we use BOW. It’s like a literal bag-of-words: it only tells\n",
    "you what words occur in the document, not where they occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e9749",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Explain Bag of N-Grams ?\n",
    "\n",
    "Ans: A Bag-of-N-Grams model is a way to represent a document, similar to a [bag-of-words][/terms/bag-of-words/] model.\n",
    "\n",
    "A bag-of-n-grams model represents a text document as an unordered collection of its n-grams.\n",
    "\n",
    "For example, let’s use the following phrase and divide it into bi-grams (n=2).\n",
    "\n",
    "James is the best person ever.\n",
    "\n",
    "becomes\n",
    "\n",
    "<start>James\n",
    "James is\n",
    "is the\n",
    "the best\n",
    "best person\n",
    "person ever.\n",
    "ever.<end>\n",
    "\n",
    "In a typical bag-of-n-grams model, these 6 bigrams would be a sample from a large number of bigrams observed in a corpus. And\n",
    "then James is the best person ever. would be encoded in a representation showing which of the corpus’s bigrams were observed in\n",
    "the sentence.\n",
    "\n",
    "A bag-of-n-grams model has the simplicity of the bag-of-words model, but allows the preservation of more word locality\n",
    "information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Explain TF-IDF ?\n",
    "\n",
    "Ans: TF-IDF stands for “Term Frequency – Inverse Document Frequency.” It reflects how important a word is to a document in a \n",
    "    collection or corpus. This technique is often used in information retrieval and text mining as a weighing factor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e834671",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What is OOV problem?\n",
    "\n",
    "Ans: Out-Of-Vocabulary (OOV) words is an important problem in NLP, we will introduce how to process words that are out of \n",
    "    vocabulary in this tutorial.\n",
    "\n",
    "We often use word2vec or glove to process documents to create word vector or word embedding.However, we may ignore some words\n",
    "that appear rarely in documents, which may cause OOV problem. Meanwhile, we may use some pre-trained word representation file, \n",
    "which may do not contain some words in our data set. It also can cause OOV problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd03cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What are word embeddings?\n",
    "\n",
    "Ans: Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of \n",
    "    a word in a document, semantic and syntactic similarity, relation with other words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Explain Continuous bag of words (CBOW)\n",
    "Ans: The word2vec model has two different architectures to create the word embeddings. They are:\n",
    "\n",
    "Continuous bag of words(CBOW)\n",
    "Skip-gram model\n",
    "\n",
    "The CBOW model tries to understand the context of the words and takes this as input. It then tries to predict words that are \n",
    "cotextually accurate. Let us consider an example for understanding this. Consider the sentence: It is a pleasant day and the\n",
    "word pleasant goes as input to the neural network. We are trying to predict the word day here. We will use the one-hot \n",
    "encoding for the input words and measure the error rates with the one-hot encoded target word. Doing this will help us \n",
    "predict the output based on the word with least error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd2401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Explain SkipGram\n",
    "\n",
    "Ans: The Skip-gram Model: The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does.\n",
    "    It tries to predict the source context words (surrounding words) given a target word (the center word). Considering our\n",
    "    simple sentence from earlier, “the quick brown fox jumps over the lazy dog”. If we used the CBOW model, we get pairs of\n",
    "    (context_window, target_word)where if we consider a context window of size 2, we have examples like ([quick, fox], brown),\n",
    "    ([the, brown], quick), ([the, dog], lazy) and so on. Now considering that the skip-gram model’s aim is to predict the \n",
    "    context from the target word, the model typically inverts the contexts and targets, and tries to predict each context word\n",
    "    from its target word. Hence the task becomes to predict the context [quick, fox] given target word ‘brown’ or [the, brown]\n",
    "    given target word ‘quick’ and so on. Thus the model tries to predict the context_window words based on the target_word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2129ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Explain Glove Embeddings.\n",
    "Ans: GloVe word is a combination of two words- Global and Vectors. In-depth, the GloVe is a model used for the representation\n",
    "    of the distributed words. This model represents words in the form of vectors using an unsupervised learning algorithm. \n",
    "    This unsupervised learning algorithm maps the words into space where the semantic similarity between the words is observed\n",
    "    by the distance between the words. These algorithms perform the Training of a corpus consisting of the aggregated global \n",
    "    word-word co-occurrence statistics, and the result of the training usually represents the subspace of the words in which\n",
    "    our interest lies. It is developed as an open-source project at Stanford and was launched in 2014.\n",
    "    \n",
    "    Training Procedure for GloVe Model:\n",
    "\n",
    "The glove model uses the matrix factorization technique for word embedding on the word-context matrix. It starts working by \n",
    "building a large matrix which consists of the words co-occurrence information, basically, The idea behind this matrix is to\n",
    "derive the relationship between the words from statistics. The co-occurrence matrix tells us the information about the\n",
    "occurrence of the words in different pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af50e969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
